{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Othello-GPT and save to `ckpts`\n",
    "\n",
    "Use `jupyter nbconvert --execute --to notebook --allow-errors --ExecutePreprocessor.timeout=-1 train_gpt_othello.ipynb --inplace --output ckpts/checkpoint.ipynb` to run in background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from data import get_othello\n",
    "from data.othello import permit, start_hands, OthelloBoardState, permit_reverse\n",
    "from mingpt.dataset import CharDataset\n",
    "from mingpt.utils import sample\n",
    "from mingpt.model import GPT, GPTConfig\n",
    "from mingpt.trainer import Trainer, TrainerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_or_championship = True  # True for training on the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPSData:\n",
    "    def __init__(self, data_path):\n",
    "        self.sequences = self.get_sequences(data_path)\n",
    "    def get_sequences(self, data_path):\n",
    "        with open(data_path, \"r\") as f:\n",
    "            data = f.readlines()\n",
    "        return [s.replace(\"\\n\", \"\").split(\" \") for s in data]\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created has 17792 sequences, 53 unique words.\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/Users/davidsewell/Projects/open_spiel/notebook/rps_tourney.txt\"\n",
    "\n",
    "rps_data = RPSData(data_path)\n",
    "\n",
    "train_dataset = CharDataset.load_rps_data(rps_data)\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, n_layer=8, n_head=8, n_embd=128)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#othello = get_othello(ood_num=-1, data_root=None if synthetic_or_championship else \"data/othello_championship\", wthor=True)\n",
    "#train_dataset = CharDataset(othello)\n",
    "#mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, n_layer=8, n_head=8, n_embd=512)\n",
    "#model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_20230526_220525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 34: train loss 2.51305. lr 1.000000e-04: 100%|█████████████████████████████████████████| 35/35 [35:48<00:00, 61.38s/it]\n",
      "epoch 2 iter 34: train loss 1.73731. lr 2.000000e-04: 100%|█████████████████████████████████████████| 35/35 [16:14<00:00, 27.85s/it]\n",
      "epoch 3 iter 34: train loss 1.18844. lr 3.000000e-04: 100%|█████████████████████████████████████████| 35/35 [16:20<00:00, 28.01s/it]\n",
      "epoch 4 iter 34: train loss 1.06038. lr 4.000000e-04: 100%|█████████████████████████████████████████| 35/35 [16:21<00:00, 28.05s/it]\n",
      "epoch 5 iter 34: train loss 1.05481. lr 5.000000e-04: 100%|█████████████████████████████████████████| 35/35 [16:13<00:00, 27.82s/it]\n",
      "epoch 6 iter 34: train loss 0.97667. lr 4.999794e-04: 100%|█████████████████████████████████████████| 35/35 [16:02<00:00, 27.49s/it]\n",
      "epoch 7 iter 34: train loss 0.98106. lr 4.999178e-04: 100%|█████████████████████████████████████████| 35/35 [15:54<00:00, 27.27s/it]\n",
      "epoch 8 iter 34: train loss 0.96858. lr 4.998150e-04: 100%|█████████████████████████████████████████| 35/35 [16:00<00:00, 27.45s/it]\n",
      "epoch 9 iter 34: train loss 0.94750. lr 4.996712e-04: 100%|█████████████████████████████████████████| 35/35 [16:02<00:00, 27.50s/it]\n",
      "epoch 10 iter 34: train loss 0.91289. lr 4.994863e-04: 100%|████████████████████████████████████████| 35/35 [16:00<00:00, 27.45s/it]\n",
      "epoch 11 iter 34: train loss 0.90520. lr 4.992605e-04: 100%|█████████████████████████████████████| 35/35 [6:03:18<00:00, 622.80s/it]\n",
      "epoch 12 iter 34: train loss 0.86469. lr 4.989936e-04: 100%|████████████████████████████████████████| 35/35 [17:06<00:00, 29.32s/it]\n",
      "epoch 13 iter 34: train loss 0.84790. lr 4.986858e-04: 100%|████████████████████████████████████████| 35/35 [16:34<00:00, 28.42s/it]\n",
      "epoch 14 iter 34: train loss 0.85906. lr 4.983370e-04: 100%|████████████████████████████████████████| 35/35 [16:53<00:00, 28.96s/it]\n",
      "epoch 15 iter 34: train loss 0.82985. lr 4.979475e-04: 100%|████████████████████████████████████████| 35/35 [16:39<00:00, 28.57s/it]\n",
      "epoch 16 iter 34: train loss 0.81367. lr 4.975172e-04: 100%|████████████████████████████████████████| 35/35 [16:43<00:00, 28.66s/it]\n",
      "epoch 17 iter 34: train loss 0.79692. lr 4.970462e-04: 100%|████████████████████████████████████████| 35/35 [50:24<00:00, 86.40s/it]\n",
      "epoch 18 iter 34: train loss 0.79522. lr 4.965346e-04: 100%|████████████████████████████████████████| 35/35 [16:27<00:00, 28.22s/it]\n",
      "epoch 19 iter 34: train loss 0.81344. lr 4.959824e-04: 100%|████████████████████████████████████████| 35/35 [16:15<00:00, 27.86s/it]\n",
      "epoch 20 iter 22: train loss 0.79692. lr 4.955947e-04:  66%|██████████████████████████▎             | 23/35 [19:39<06:00, 30.05s/it]"
     ]
    }
   ],
   "source": [
    "max_epochs = 250\n",
    "# initialize a trainer instance and kick off training\n",
    "t_start = time.strftime(\"_%Y%m%d_%H%M%S\")\n",
    "tconf = TrainerConfig(\n",
    "    max_epochs=max_epochs, \n",
    "    batch_size=512*1,  # assuming 8 GPU's\n",
    "    learning_rate=5e-4,\n",
    "    lr_decay=True, \n",
    "    warmup_tokens=len(train_dataset)*train_dataset.block_size*5, \n",
    "    final_tokens=len(train_dataset)*train_dataset.block_size*max_epochs,\n",
    "    num_workers=0, \n",
    "    ckpt_path=f\"./ckpts/gpt_at{t_start}.ckpt\", \n",
    ")\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "device = trainer.device\n",
    "print(t_start)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or load trained model from `ckpts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_res = model.load_state_dict(torch.load(\"./ckpts/gpt_at_20230526_170210.ckpt\"))\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(model, data,idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        idx_cond = idx if idx.size(1) <= data.block_size else idx[:, -data.block_size:]\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits, _ = model(idx_cond)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # either sample from the distribution or take the most likely element\n",
    "        if do_sample:\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "        # append sampled index to the running sequence and continue\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def generate(model, data, prompt='', num_samples=1, steps=100, do_sample=True):\n",
    "        \n",
    "    dix = [train_dataset.stoi[s] for s in prompt]\n",
    "    \n",
    "    x = torch.tensor(dix, dtype=torch.long)\n",
    "    # we'll process all desired num_samples in a batch, so expand out the batch dim\n",
    "    x = x.expand(num_samples, -1)\n",
    "\n",
    "    # forward the model `steps` times to get samples, in a batch\n",
    "    sample = get_sample(model, data, x, steps, do_sample=do_sample, top_k=None)\n",
    "\n",
    "    return sample\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ['rockbot', 'greenberg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dix = [train_dataset.stoi[s] for s in prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42, 24]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = generate(model, train_dataset, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rockbot',\n",
       " 'greenberg',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[train_dataset.itos[int(s)] for s in sample[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
